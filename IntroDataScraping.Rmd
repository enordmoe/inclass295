---
title: "Introduction to Data Scraping with R"
author: "Your Name Here"
output: 
  github_document: 
    toc: true
    toc_depth: 2
---

# 1. Intro  

Web pages are written in HTML (Hyper Text Markup Language) which uses **tags** to describe different aspects of document content. For example, a heading in a document is indicated by `<h1>My Title</h1>` whereas a paragraph would be indicated by `<p>A paragraph of content...</p>`. 


In this tutorial, we will learn how to read data from a table on a web page into R. We will need the package `rvest` to get the data from the web page, and the `stringr` package to clean up the data.

```{r, setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(rvest)
library(magrittr) # provides extra commands like `extract` and `extract2` that allow easier use of the pipe

```


# 2. A movie box office example


The web site [Box Office Mojo](http://www.boxofficemojo.com) gives statistics on box office earnings of movies. In addition to daily earnings, the web site also maintains lists of yearly and all time record holders.


We will look at the movies in the top 100 in all time movie worldwide grosses in box office receipts. In particular, we will scrape the data from [Box Office Mojo: All Time Box Office](http://www.boxofficemojo.com/alltime/world/?pagenum=1).

* Use SelectorGadget to identify the css selectors and get the variables separately.  
* Use `tibble()` to combine the vectors into a tibble dataset.  

```{r}
movieParse<- read_html("https://www.boxofficemojo.com/chart/top_lifetime_gross/?area=XWW")

# Get rank column vector
rank <- movieParse %>% 
  html_nodes(".mojo-truncate") %>% 
  html_text()

# Get title column vector
title <- movieParse %>% 
  html_nodes(".mojo-field-type-title") %>% 
  html_text() %>% 
  extract(-1) #remove first row with title name

# Get Lifetime gross column vector
gross <- movieParse %>% 
  html_nodes(".mojo-field-type-money") %>% 
  html_text() %>% 
  extract(-1) #remove first row with title name

# Get year released  vector
year <- movieParse %>% 
  html_nodes(".mojo-field-type-year") %>% 
  html_text() %>% 
  extract(-1) #remove first row with title name


movies <- tibble(rank, title, gross, year)

movies
```


Convert the character variables into numeric variables and save in **movies2**. 


```{r}
movies2 <- movies %>%
  mutate(rank = parse_number(rank),
    gross = parse_number(gross),
    year = parse_number(year)
  )
head(movies2)
```

**On your own:** Try another Box Office mojo list like the on here:  

https://www.boxofficemojo.com/year/world/2019/

# 3.  A Billboard Top 100 example

The website [billboard.com](http://www.billboard.com)
keeps track of top songs, albums and artists from the music industry.

One page lists the hot 100 artists. We can use SelectorGadget to verify that the best css selector for artist name is is ".chart-list-item__title". Use the pipe to access the list of artists and use `trim = TRUE` in `html_text()` to clean up the text extracted from the node. 

```{r}
billboard <- read_html("https://www.billboard.com/charts/artist-100")

artist <- billboard %>% 
  html_nodes(".chart-list-item__title") %>%
  html_text(trim = TRUE)

head(artist)
```

**On your own**: Try another Billboard top 100 list to see if you can adapt this technique to a similar page.


# 4. Scraping Population Data from Wikipedia


Data on the web are often presented in tables. For instance, we can see a list of countries by population in 1900 on [Wikipedia](https://en.wikipedia.org/wiki/List_of_countries_by_population_in_1900)




```{r, message = FALSE}
library(rvest)
library(tidyverse)
library(magrittr)
```


## a) Reading data into R with `rvest`


To get the population data on [Wikipedia](https://en.wikipedia.org/wiki/List_of_countries_by_population_in_1900) into R, we use the `read_html` command from the `xml2` package (which is attached when `rvest` is called) to parse the page to obtain an HTML document. 


We then use the `html_nodes` command that extracts all occurrences of the desired tag. We will be interested in scraping data presented in tables, so in the source code, we look for the table tag: `<table> ... </table>`.


Note: some of the `rvest` commands may be slow depending on your Internet connection and the complexity of the web page.


```{r}
popParse <- read_html("https://en.wikipedia.org/wiki/List_of_countries_by_population_in_1900")
str(popParse)
```


The `read_html` command creates an R object, basically a list, that stores information about the web page.


To extract a table from this web page (this may take some time):


```{r}
popNodes <- html_nodes(popParse, "table")
popNodes
```
There are several tables in this document. By inspecting the output of `popNodes`, we make a guess that we want the sixth table. (Trial and error may be required.) We select the sixth table by using double brackets because we want the content of the 6th item from a list (more on R lists soon):


```{r}
pop <- html_table(popNodes, header = TRUE, fill = TRUE)[[6]]
str(pop) # take a look at the structure
```

## b) Cleaning the data frame


We now have a workable data frame that we can clean using the tidyverse techniques we have been learning. Notice that all the variables have been read in as characters but we would typically regard rank, population and percentages as numeric   For `Rank`, that is because the first observation is the world population and it is not assigned a rank, but rather, the character "-". The population and percentage columns are seen as characters because they contain commas and percent signs as well as occasional explantory notes. We need to clean up the data and convert these columns to be numeric.

Use **tidyverse** and **janitor**  commands to create a tidy data set called **popdata** that incorporates the following:
  * Removes the total world population row  
  * Syntactic names are converted to legal names
  * Character variable are converted to numeric where appropriate 
  * Special characters such as bracketed footnotes have been removed to simplify the appearance.
    + The following regular expression is very useful. Can you figure out what it finds and why you might want to replace it with a ""?
    ` "\\[[^]]+\\]" `
  
You should be able to do this in one continuous pipe but you should build up to it gradually by inspecting intermediate results. Here's one approach:

```{r}
library(tidyverse)
library(janitor)
df <- tibble(pop) %>% 
  slice(-1) %>% 
  clean_names() %>%
  rename(population = population_c_1900_estimate_1) %>%
  mutate(rank = parse_number(rank, na = c("â€”")), population = parse_number(population),
         percentage_of_world_population = parse_number(percentage_of_world_population) ) %>%
  mutate(country_territory = str_replace_all(country_territory, "\\[[^]]+\\]", "" ))
df

```






# 5.  On Your Own


* The web site [BikeRaceInfo](http://www.bikeraceinfo.com/tdf/tdfstats.html) has a table with data on past winners of the Tour de France. Create a cleaned-up data frame of this data.

* The web site [NY Times Best Sellers: Hardcover Fiction](http://www.nytimes.com/books/best-sellers/hardcover-fiction) contains a list of best-selling fiction books. Scrape the names of these top books. Use SelectorGadget to obtain the appropriate css selector. 


# 6. Resources

* [HTML Tutorial](www.w3schools.com)